{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817cfd59-a54b-4394-9656-9d0e79711d0c",
   "metadata": {},
   "source": [
    "## **Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc69a03-0055-4ae8-91d9-52ebb078f872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from urllib.parse import quote_plus\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from config import oltp_conn_string\n",
    "from config import warehouse_conn_string\n",
    "from config import etl_config\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe9f03-4267-48de-9efa-696ba9844d82",
   "metadata": {},
   "source": [
    "## **Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9e2a6fa-ba4a-4d7d-93ae-70b3721c32bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def db_connection(conn_params):\n",
    "    \"\"\"Create a connection engine to the database\"\"\"\n",
    "    conn_str = f\"postgresql://{conn_params['user']}:{quote_plus(conn_params['password'])}@{conn_params['host']}:{conn_params['port']}/{conn_params['database']}\"\n",
    "    engine = create_engine(conn_str)\n",
    "    return engine.connect()\n",
    "\n",
    "def validate_config(etl_config):\n",
    "    \"\"\"Validate the ETL configuration\"\"\"\n",
    "    required_keys = ['source_table', 'query', 'destination_table', 'column_mapping']\n",
    "    for table_name, table_config in etl_config.items():\n",
    "        for key in required_keys:\n",
    "            if key not in table_config:\n",
    "                raise ValueError(f\"Missing {key} in config for table {table_name}\")\n",
    "    logging.info(\"Config validation passed\")\n",
    "\n",
    "def extract(table_config):\n",
    "    \"\"\"Extract data from the source table\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Extracting data from {table_config['source_table']}...\")\n",
    "        with db_connection(oltp_conn_string) as conn:\n",
    "            df = pd.read_sql(table_config[\"query\"], conn)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting data from {table_config['source_table']}: {e}\")\n",
    "        raise\n",
    "\n",
    "def transform(df, table_config):\n",
    "    \"\"\"Transform the extracted data\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Transforming data for {table_config['destination_table']}...\")\n",
    "        df.rename(columns=table_config[\"column_mapping\"], inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error transforming data for {table_config['destination_table']}: {e}\")\n",
    "        raise\n",
    "\n",
    "def load(df, table_config):\n",
    "    \"\"\"Load the transformed data into the destination table, replacing the data without dropping the table\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Replacing data in {table_config['destination_table']}...\")\n",
    "\n",
    "        # Connect to the warehouse database\n",
    "        with db_connection(warehouse_conn_string) as conn:\n",
    "            # Step 1: Truncate the table (remove all existing data)\n",
    "            conn.execute(f\"TRUNCATE TABLE {table_config['destination_table']} RESTART IDENTITY CASCADE;\")\n",
    "            \n",
    "            # Step 2: Insert the new data into the table using append (this won't drop the table)\n",
    "            df.to_sql(\n",
    "                table_config[\"destination_table\"], \n",
    "                conn, \n",
    "                if_exists=\"append\",  # This will replace new data\n",
    "                index=False\n",
    "            )\n",
    "        logging.info(f\"Data successfully loaded into {table_config['destination_table']}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error replacing data in {table_config['destination_table']}: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_etl():\n",
    "    \"\"\"Run the full ETL process.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting ETL Process...\")\n",
    "        validate_config(etl_config)  # Validate config\n",
    "        for table_name, table_config in etl_config.items():\n",
    "            df = extract(table_config)\n",
    "            df = transform(df, table_config)\n",
    "            load(df, table_config)\n",
    "        logging.info(\"ETL Process Completed Successfully!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ETL process failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea0141-cfa2-4c9c-a9da-55eb669597c6",
   "metadata": {},
   "source": [
    "## **Run Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e58e8c7-0baa-4bed-835d-7b6262a7b6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting ETL Process...\n",
      "INFO:root:Config validation passed\n",
      "INFO:root:Extracting data from tb_users...\n",
      "INFO:root:Transforming data for dim_user...\n",
      "INFO:root:Replacing data in dim_user...\n",
      "INFO:root:Data successfully loaded into dim_user\n",
      "INFO:root:Extracting data from tb_payments...\n",
      "INFO:root:Transforming data for dim_payment...\n",
      "INFO:root:Replacing data in dim_payment...\n",
      "INFO:root:Data successfully loaded into dim_payment\n",
      "INFO:root:Extracting data from tb_shippers...\n",
      "INFO:root:Transforming data for dim_shipper...\n",
      "INFO:root:Replacing data in dim_shipper...\n",
      "INFO:root:Data successfully loaded into dim_shipper\n",
      "INFO:root:Extracting data from tb_ratings...\n",
      "INFO:root:Transforming data for dim_rating...\n",
      "INFO:root:Replacing data in dim_rating...\n",
      "INFO:root:Data successfully loaded into dim_rating\n",
      "INFO:root:Extracting data from tb_vouchers...\n",
      "INFO:root:Transforming data for dim_voucher...\n",
      "INFO:root:Replacing data in dim_voucher...\n",
      "INFO:root:Data successfully loaded into dim_voucher\n",
      "INFO:root:Extracting data from ['tb_orders', 'tb_users', 'tb_payments', 'tb_shippers', 'tb_ratings', 'tb_vouchers']...\n",
      "INFO:root:Transforming data for fact_orders...\n",
      "INFO:root:Replacing data in fact_orders...\n",
      "INFO:root:Data successfully loaded into fact_orders\n",
      "INFO:root:ETL Process Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_etl()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
